{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/Epics_Main_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I deposited ₹55905 in my savings account</td>\n",
       "      <td>Amount</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My mobile no. is 8634525228</td>\n",
       "      <td>Phone Number</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Call me on 7662180703</td>\n",
       "      <td>Phone Number</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I deposited 95759 INR in my savings account</td>\n",
       "      <td>Amount</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Call me on 6862714992</td>\n",
       "      <td>Phone Number</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          text         label\n",
       "0     I deposited ₹55905 in my savings account        Amount\n",
       "1                  My mobile no. is 8634525228  Phone Number\n",
       "2                        Call me on 7662180703  Phone Number\n",
       "3  I deposited 95759 INR in my savings account        Amount\n",
       "4                        Call me on 6862714992  Phone Number"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count      19372\n",
       "unique         4\n",
       "top       Amount\n",
       "freq        4904\n",
       "Name: label, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19372, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import contractions\n",
    "from word2number import w2n\n",
    "from num2words import num2words\n",
    "# from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download stopwords for the first time\n",
    "# nltk.download(\"stopwords\")\n",
    "# nltk.download(\"punkt\")\n",
    "# nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading stopwords\n",
    "# stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number word mapping for conversions (expandable)\n",
    "number_mapping = {\n",
    "    \"k\": \"000\",   # 5k -> 5000\n",
    "    \"m\": \"000000\",  # 2m -> 2000000\n",
    "    \"b\": \"000000000\",  # 3b -> 3000000000\n",
    "    \"lakh\": \"00000\",  # 10 lakh -> 1000000\n",
    "    \"crore\": \"0000000\",  # 2 crore -> 20000000\n",
    "    \"million\": \"000000\",  # half million -> 500000\n",
    "    \"billion\": \"000000000\"  # 3 billion -> 3000000000\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_words_to_numbers(text):\n",
    "    \"\"\"\n",
    "    Convert spoken numbers (e.g., 'five thousand' → '5000') & handle 5k, 10 lakh, etc.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    processed_words = []\n",
    "    temp_phrase = \"\"\n",
    "\n",
    "    for word in words:\n",
    "        # Handle abbreviations like \"5k\" → \"5000\"\n",
    "        for key, value in number_mapping.items():\n",
    "            if word.endswith(key):\n",
    "                num_part = re.sub(r\"\\D\", \"\", word)  # Extract numeric part\n",
    "                if num_part:\n",
    "                    processed_words.append(num_part + value)\n",
    "                    temp_phrase = \"\"  # Clear the phrase\n",
    "                break\n",
    "        else:\n",
    "            # Accumulate words to form a numeric phrase\n",
    "            temp_phrase += f\" {word}\"\n",
    "            try:\n",
    "                # Attempt to convert accumulated words to a number\n",
    "                num_value = w2n.word_to_num(temp_phrase.strip())\n",
    "                processed_words.append(str(num_value))\n",
    "                temp_phrase = \"\"  # Clear after conversion\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "    # Append any remaining phrase\n",
    "    if temp_phrase.strip():\n",
    "        processed_words.extend(temp_phrase.strip().split())\n",
    "\n",
    "    return \" \".join(processed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess input text:\n",
    "    1. Expand contractions (e.g., \"I'm\" → \"I am\")\n",
    "    2. Convert to lowercase\n",
    "    3. Remove punctuation & special characters\n",
    "    4. Convert numbers (e.g., '5k' → '5000')\n",
    "    5. Remove stopwords (optional)\n",
    "    6. Handle multiple spaces\n",
    "    \"\"\"\n",
    "\n",
    "    # Expand contractions (e.g., \"I'm\" → \"I am\")\n",
    "    text = contractions.fix(text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove special characters & punctuation (except numbers & words)\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "\n",
    "    # Convert numbers in words and handle 5k, 10 lakh, etc.\n",
    "    text = convert_words_to_numbers(text)\n",
    "\n",
    "    # Remove stopwords (optional)\n",
    "    # words = word_tokenize(text)\n",
    "    # text = \" \".join([word for word in words if word not in stop_words])\n",
    "\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0            i deposited 55905 in my savings account\n",
      "1                         my mobile no is 8634525228\n",
      "2                              call me on 7662180703\n",
      "3        i deposited 95759 inr in my savings account\n",
      "4                              call me on 6862714992\n",
      "                            ...                     \n",
      "19367                             ac no 827137161025\n",
      "19368                   here is my number 6626638399\n",
      "19369                   here is my number 9836564261\n",
      "19370              my account number is 110449552902\n",
      "19371             i would like to withdraw 38 rupees\n",
      "Name: text, Length: 19372, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Apply Preprocessing\n",
    "df[\"text\"] = df[\"text\"].apply(preprocess_text)\n",
    "\n",
    "print(df[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i deposited 55905 in my savings account'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38744"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making train-test split\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(df[\"text\"], df[\"label\"], test_size=0.2, random_state=42)\n",
    "\n",
    "# Converting our data to Hugging Face Dataset format(gives better results while using big dataset)\n",
    "train_dataset = Dataset.from_dict({\"text\": train_texts.tolist(), \"label\": train_labels.tolist()})\n",
    "val_dataset = Dataset.from_dict({\"text\": val_texts.tolist(), \"label\": val_labels.tolist()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 15497/15497 [00:01<00:00, 8444.18 examples/s]\n",
      "Map: 100%|██████████| 3875/3875 [00:00<00:00, 9783.65 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Define label mapping\n",
    "label_mapping = {\n",
    "    \"Name\": 0,\n",
    "    \"Phone Number\": 1,\n",
    "    \"Amount\": 2,\n",
    "    \"Account Number\": 3\n",
    "}\n",
    "\n",
    "# Apply mapping to dataset\n",
    "train_dataset = train_dataset.map(lambda x: {\"label\": label_mapping[x[\"label\"]]})\n",
    "val_dataset = val_dataset.map(lambda x: {\"label\": label_mapping[x[\"label\"]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 15497/15497 [00:07<00:00, 2028.77 examples/s]\n",
      "Map: 100%|██████████| 3875/3875 [00:01<00:00, 2011.91 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Load DistilBERT tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"], \n",
    "        padding=\"max_length\", \n",
    "        truncation=True,       \n",
    "        max_length=64,         \n",
    "        return_tensors=\"pt\"   \n",
    "    )\n",
    "\n",
    "# Apply tokenization\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 15497/15497 [00:04<00:00, 3853.97 examples/s]\n",
      "Map: 100%|██████████| 3875/3875 [00:00<00:00, 10087.55 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Convert labels to integers\n",
    "# train_dataset = train_dataset.map(lambda x: {\"label\": int(x[\"label\"])})\n",
    "# val_dataset = val_dataset.map(lambda x: {\"label\": int(x[\"label\"])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification, DistilBertConfig\n",
    "\n",
    "# Create a configuration with increased dropout\n",
    "config = DistilBertConfig.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=4, \n",
    "    dropout=0.3, \n",
    "    attention_dropout=0.3 \n",
    ")\n",
    "\n",
    "# Load model with the updated configuration\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to compute metrics\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    return {\"accuracy\": acc, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TensorBoard\n",
    "from transformers.integrations import TensorBoardCallback\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Initialize TensorBoard writer\n",
    "writer = SummaryWriter(log_dir=\"./tensorboard_logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nitin Mishra\\.virtualenvs\\Voice_Activated_Form_Assisstant-eTL7iws_\\Lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\Nitin Mishra\\AppData\\Local\\Temp\\ipykernel_20244\\2700627398.py:18: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01, \n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=[\"tensorboard\"],\n",
    ")\n",
    "\n",
    "# Trainer initialization\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3), TensorBoardCallback(writer)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9690' max='9690' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9690/9690 10:50:21, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.075200</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9690, training_loss=0.006636795067261701, metrics={'train_runtime': 39029.7463, 'train_samples_per_second': 1.985, 'train_steps_per_second': 0.248, 'total_flos': 1283075310128640.0, 'train_loss': 0.006636795067261701, 'epoch': 5.0})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='485' max='485' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [485/485 07:47]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'eval_loss': 8.435090421698987e-07, 'eval_accuracy': 1.0, 'eval_f1': 1.0, 'eval_runtime': 468.7786, 'eval_samples_per_second': 8.266, 'eval_steps_per_second': 1.035, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(\"Evaluation Results:\", eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Category: Amount\n"
     ]
    }
   ],
   "source": [
    "def predict(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=64)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    prediction = torch.argmax(outputs.logits, dim=1).item()\n",
    "    categories = [\"Name\", \"Phone Number\", \"Amount\", \"Account Number\"]\n",
    "    return categories[prediction]\n",
    "\n",
    "# Example Prediction\n",
    "example_text = \"Transfer 10000 rupees\"\n",
    "print(\"Predicted Category:\", predict(example_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39msummary()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Voice_Activated_Form_Assisstant-eTL7iws_",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
